---
layout: default
title: Building VFTCam
description: "A technical deep dive into building a zero-dependency 360° photosphere capture web app for virtual field trips"
---

<div class="container py-5">
  <article class="article-content mx-auto">
    <!-- Header -->
    <header class="text-center mb-5">
      <h1 class="display-4 fw-bold mb-3">Building VFTCam</h1>
      <p class="lead text-secondary">A Zero-Dependency 360° Photosphere Capture Web App</p>
      <div class="d-flex justify-content-center gap-3 mt-4 flex-wrap">
        <span class="badge" style="background-color: #B1040E;"><i class="bi bi-mortarboard me-1"></i>Stanford</span>
        <span class="badge" style="background-color: #F7DF1E;"><i class="bi bi-filetype-js me-1 text-dark"></i><span class="text-dark">JavaScript</span></span>
        <span class="badge" style="background-color: #000000;"><i class="bi bi-box me-1"></i>Three.js</span>
        <span class="badge bg-secondary"><i class="bi bi-gpu-card me-1"></i>WebGL2</span>
      </div>
    </header>

    <p class="lead text-secondary mb-5">
      VFTCam is a browser-based Progressive Web App that creates 360° panoramic photospheres using structured camera capture and GPU-accelerated stitching. Built as a spiritual successor to Google's discontinued Street View Camera app, it runs entirely in the browser with zero external dependencies and no build tools.
    </p>

    <h2><i class="bi bi-exclamation-triangle me-2"></i>The Challenge</h2>
    <p>When Google discontinued their Street View Camera app in 2023, educators lost a valuable tool for creating immersive virtual field trips. A replacement was needed that could:</p>
    <ul>
      <li>Run entirely in mobile browsers without native app installation</li>
      <li>Adopt a privacy-first approach with no user data collection</li>
      <li>Work offline in remote field locations</li>
      <li>Handle mobile device memory constraints (especially iOS Safari's ~1.4GB limit)</li>
      <li>Provide real-time guidance for capturing aligned photos</li>
      <li>Stitch 36 high-resolution images into equirectangular panoramas</li>
    </ul>

    <h2><i class="bi bi-diagram-3 me-2"></i>Architecture Overview</h2>
    <p>VFTCam is built as a "zero-build" application using pure ES6 modules. This decision prioritizes maintainability and debuggability over bundle size optimization.</p>

    <div class="row row-cols-1 row-cols-md-3 g-3 my-4">
      <div class="col">
        <div class="card h-100">
          <div class="card-body">
            <h5 class="card-title">Core Technologies</h5>
            <ul class="mb-0">
              <li>ES6 Modules (no bundler)</li>
              <li>WebGL2 for GPU processing</li>
              <li>Three.js for 3D visualization</li>
              <li>IndexedDB for image storage</li>
            </ul>
          </div>
        </div>
      </div>
      <div class="col">
        <div class="card h-100">
          <div class="card-body">
            <h5 class="card-title">Device APIs</h5>
            <ul class="mb-0">
              <li>WebRTC getUserMedia</li>
              <li>DeviceOrientationEvent</li>
              <li>DeviceMotionEvent</li>
              <li>Geolocation API</li>
            </ul>
          </div>
        </div>
      </div>
      <div class="col">
        <div class="card h-100">
          <div class="card-body">
            <h5 class="card-title">PWA Features</h5>
            <ul class="mb-0">
              <li>Service Worker (Workbox)</li>
              <li>Web App Manifest</li>
              <li>OPFS for panoramas</li>
              <li>Web Share API</li>
            </ul>
          </div>
        </div>
      </div>
    </div>

    <h2><i class="bi bi-grid-3x3 me-2"></i>The Capture Pattern</h2>
    <p>The heart of VFTCam is its structured 36-point capture pattern, arranged in three rows:</p>
    <pre class="bg-light p-3 rounded"><code>Upper Row (pitch +45°):  12 points at 30° yaw intervals
Equator (pitch 0°):      12 points at 30° yaw intervals
Lower Row (pitch -45°):  12 points at 30° yaw intervals

Total Coverage: 36 overlapping images covering full sphere</code></pre>

    <h3>Inside the Sphere</h3>
    <p>Unlike traditional panorama apps that show a flat grid or compass interface, VFTCam places the user <em>inside</em> a Three.js wireframe sphere. This transforms the abstract concept of spherical capture into an intuitive, spatial experience.</p>

    <div class="alert alert-info">
      <strong>Key Innovation:</strong> By visualizing capture points as physical locations on a sphere surrounding the user, VFTCam makes the complex mathematics of spherical projection tangible. Users can literally "see" the photosphere they're building from the inside out.
    </div>

<pre class="bg-dark text-light p-3 rounded"><code class="language-javascript">// scene.js - Creating the capture sphere environment
export class Scene {
    constructor(canvas) {
        this.scene = new THREE.Scene();
        this.camera = new THREE.PerspectiveCamera(75, aspect, 0.1, 1000);

        // Create wireframe sphere (user is INSIDE looking out)
        const sphereGeometry = new THREE.SphereGeometry(100, 24, 16);
        const sphereMaterial = new THREE.MeshBasicMaterial({
      color: 0x666666,
      wireframe: true,
      opacity: 0.3,
      transparent: true,
      side: THREE.BackSide  // CRITICAL: Render inside of sphere
        });

        this.sphere = new THREE.Mesh(sphereGeometry, sphereMaterial);
        this.scene.add(this.sphere);
    }
}</code></pre>

    <h2><i class="bi bi-camera-reels me-2"></i>Understanding Field of View</h2>
    <p>FOV calculation was one of the trickiest aspects. Getting it wrong means gaps in coverage or wasted overlap.</p>

    <div class="row my-4">
      <div class="col-md-6">
        <div class="alert alert-danger">
          <strong>Challenge:</strong> WebRTC's getUserMedia captures video streams, not photos. The FOV for video mode is often different from the device's photo capture mode—typically narrower due to cropping and stabilization.
        </div>
      </div>
      <div class="col-md-6">
        <div class="alert alert-success">
          <strong>Solution:</strong> Through extensive empirical testing across devices, we found 44° horizontal FOV provided the best balance. With 30° spacing between capture points, this gives ~14° overlap (~32%).
        </div>
      </div>
    </div>

    <h2><i class="bi bi-layers me-2"></i>WebGL2 Best-Pixel Stitching</h2>
    <p>The stitching algorithm runs entirely on the GPU using WebGL2 fragment shaders. Each output pixel samples from all overlapping source images and selects the "best" one based on multiple quality metrics:</p>

    <ul>
      <li><strong>Distance from center:</strong> Pixels near image center have less lens distortion</li>
      <li><strong>Sharpness:</strong> Laplacian-based edge detection identifies blur</li>
      <li><strong>Angular distance:</strong> Pixels closest to their source image's center direction</li>
      <li><strong>Exposure consistency:</strong> Weighted toward median brightness across images</li>
    </ul>

<pre class="bg-dark text-light p-3 rounded"><code class="language-glsl">// Fragment shader for best-pixel selection
void main() {
    vec2 equirect = gl_FragCoord.xy / resolution;
    vec3 sphereDir = equirectToSphere(equirect);

    float bestScore = -1.0;
    vec4 bestColor = vec4(0.0);

    for (int i = 0; i < NUM_IMAGES; i++) {
        vec2 imgUV = projectToImage(sphereDir, imageOrientations[i]);
        if (isValidUV(imgUV)) {
      float score = calculateQuality(imgUV, i);
      if (score > bestScore) {
          bestScore = score;
          bestColor = texture(images[i], imgUV);
      }
        }
    }

    fragColor = bestColor;
}</code></pre>

    <h2><i class="bi bi-memory me-2"></i>Memory Management</h2>
    <p>Mobile Safari has strict memory limits (~1.4GB). With 36 high-resolution images plus WebGL textures, memory management is critical.</p>

    <div class="row row-cols-1 row-cols-md-2 g-3 my-4">
      <div class="col">
        <div class="card h-100 border-primary">
          <div class="card-body">
            <h5 class="card-title text-primary">Texture Streaming</h5>
            <p class="card-text">Only 4-6 images loaded as GPU textures at once. Images swap in/out based on which output region is being processed.</p>
          </div>
        </div>
      </div>
      <div class="col">
        <div class="card h-100 border-primary">
          <div class="card-body">
            <h5 class="card-title text-primary">Tiled Rendering</h5>
            <p class="card-text">Output renders in 512×512 tiles. Each tile completes and copies to CPU before the next starts, preventing GPU memory accumulation.</p>
          </div>
        </div>
      </div>
      <div class="col">
        <div class="card h-100 border-primary">
          <div class="card-body">
            <h5 class="card-title text-primary">Aggressive Cleanup</h5>
            <p class="card-text">Explicit WebGL resource deletion after each tile. Force garbage collection hints between major operations.</p>
          </div>
        </div>
      </div>
      <div class="col">
        <div class="card h-100 border-primary">
          <div class="card-body">
            <h5 class="card-title text-primary">IndexedDB Offloading</h5>
            <p class="card-text">Source images stored in IndexedDB immediately after capture. Only loaded into memory when needed for stitching.</p>
          </div>
        </div>
      </div>
    </div>

    <h2><i class="bi bi-arrow-clockwise me-2"></i>Crash Recovery</h2>
    <p>Mobile browsers can terminate apps at any time. VFTCam implements "stitch jobs" - resumable processing that survives app crashes:</p>

<pre class="bg-dark text-light p-3 rounded"><code class="language-javascript">// Stitch job persistence
const stitchJob = {
    id: crypto.randomUUID(),
    createdAt: Date.now(),
    status: 'pending',
    progress: { currentTile: 0, totalTiles: 64 },
    imageIds: [...capturedImageIds],
    outputTiles: []  // Completed tiles stored as blobs
};

// Save after each tile completes
async function saveTileProgress(tileIndex, tileBlob) {
    stitchJob.outputTiles[tileIndex] = await blobToBase64(tileBlob);
    stitchJob.progress.currentTile = tileIndex + 1;
    await db.put('stitchJobs', stitchJob);
}</code></pre>

    <h2><i class="bi bi-badge-vr me-2"></i>VR Viewing</h2>
    <p>Completed photospheres can be viewed in VR mode using Google Cardboard or similar headsets. The viewer uses WebXR when available, with a fallback to gyroscope-controlled stereoscopic rendering.</p>

    <h2><i class="bi bi-shield-check me-2"></i>Privacy by Design</h2>
    <p>VFTCam collects zero user data:</p>
    <ul>
      <li>No analytics or tracking scripts</li>
      <li>No server-side processing - everything runs locally</li>
      <li>No account required</li>
      <li>GPS data is optional and only embedded in exported files</li>
      <li>Images never leave the device unless explicitly shared</li>
    </ul>

    <h2><i class="bi bi-speedometer2 me-2"></i>Performance</h2>
    <div class="table-responsive">
      <table class="table table-bordered">
        <thead class="table-light">
          <tr>
            <th>Metric</th>
            <th>iPhone 13</th>
            <th>Pixel 6</th>
            <th>iPad Pro</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Capture time (36 photos)</td>
            <td>2-3 min</td>
            <td>2-3 min</td>
            <td>2-3 min</td>
          </tr>
          <tr>
            <td>Stitch time (4K output)</td>
            <td>~45 sec</td>
            <td>~60 sec</td>
            <td>~30 sec</td>
          </tr>
          <tr>
            <td>Memory peak</td>
            <td>~800 MB</td>
            <td>~900 MB</td>
            <td>~1.1 GB</td>
          </tr>
          <tr>
            <td>Output resolution</td>
            <td colspan="3" class="text-center">4096 × 2048 (4K equirectangular)</td>
          </tr>
        </tbody>
      </table>
    </div>

    <h2><i class="bi bi-lightbulb me-2"></i>Lessons Learned</h2>
    <ol>
      <li><strong>Test on real devices early.</strong> The iOS Safari memory limit shaped the entire architecture. Emulators don't expose these constraints.</li>
      <li><strong>WebGL debugging is hard.</strong> GPU errors are often silent. Build extensive logging and visualization tools for shader development.</li>
      <li><strong>Device orientation APIs vary wildly.</strong> The same code produces different results on iOS vs Android. Abstract these early.</li>
      <li><strong>Users will hold their phones wrong.</strong> Design for imperfection. Capture the actual orientation and compensate in software.</li>
      <li><strong>Zero-build has real benefits.</strong> No webpack config to debug. No node_modules. Just open the HTML file.</li>
    </ol>

    <h2><i class="bi bi-box-seam me-2"></i>External Libraries</h2>
    <ul>
      <li><a href="https://threejs.org/" target="_blank">Three.js</a> - 3D visualization and WebGL abstraction</li>
      <li><a href="https://pannellum.org/" target="_blank">Pannellum</a> - Panorama viewing</li>
      <li><a href="https://developers.google.com/web/tools/workbox" target="_blank">Workbox</a> - Service worker and PWA caching</li>
      <li><a href="https://github.com/nickyout/gyronorm.js" target="_blank">GyroNorm.js</a> - Cross-platform device orientation normalization</li>
    </ul>

    <h2><i class="bi bi-check-circle me-2"></i>Conclusion</h2>
    <p>VFTCam demonstrates that sophisticated image processing applications can run entirely in the browser, without native code, server infrastructure, or complex build systems. By embracing web standards and working within browser constraints rather than fighting them, it's possible to build tools that are both powerful and accessible.</p>
    <p>The app continues to be used by educators worldwide to create virtual field trips, bringing remote locations into classrooms and enabling students to explore places they could never physically visit.</p>

    <!-- Back to Projects -->
    <div class="text-center mt-4">
      <a href="{{ site.baseurl }}/" class="btn btn-outline-secondary">
        <i class="bi bi-arrow-left me-2"></i>Back to Projects
      </a>
      <a href="https://vftcam.stanford.edu" target="_blank" class="btn btn-primary ms-2">
        <i class="bi bi-box-arrow-up-right me-2"></i>Try VFTCam
      </a>
    </div>
  </article>
</div>

<style>
.article-content h2 {
  margin-top: 2.5rem;
  margin-bottom: 1rem;
  padding-bottom: 0.5rem;
  border-bottom: 2px solid var(--border-color);
}

.article-content h3 {
  margin-top: 1.5rem;
  margin-bottom: 1rem;
}

.article-content pre {
  margin: 1rem 0;
  overflow-x: auto;
}

.article-content pre code {
  font-size: 0.875rem;
}
</style>
